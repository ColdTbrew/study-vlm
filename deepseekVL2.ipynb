{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676adb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepSeek-VL2 model: deepseek-ai/deepseek-vl-1.3b-chat...\n",
      "--- DeepSeek-VL2 Test ---\n",
      "Caption: The image captures a serene scene of a rocky shore at sunset. The sky, awash with hues of pink and orange, serves as a stunning backdrop to the tranquil water. The rocks, varying in size and shape, are scattered across the water, their gray and black colors contrasting beautifully with the vibrant sky. The perspective of the image is from the shore, looking out towards the water, giving a sense of depth and distance. In the distance, the silhouette of mountains can be seen, adding a sense of grandeur to the scene. The image does not provide any specific details that could be used to identify the landmark as 'sa_1300'.\n",
      "Answer: The main object in the picture is a group of rocks or boulders floating on water.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
    "from deepseek_vl.utils.io import load_pil_images\n",
    "\n",
    "# 0. 사전 준비: deepseek-vl 패키지 설치 필요\n",
    "# 터미널에서 실행: pip install deepseek-vl\n",
    "\n",
    "# 1. 모델 및 프로세서 로드\n",
    "# \"deepseek-ai/deepseek-vl-7b-chat\" 또는 작은 모델 \"deepseek-ai/deepseek-vl-1.3b-chat\" 선택\n",
    "model_path = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "print(f\"Loading DeepSeek-VL2 model: {model_path}...\")\n",
    "# bfloat16을 지원하는 GPU라면 torch.bfloat16 사용 권장\n",
    "model: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# 2. 대화 템플릿 구성\n",
    "def run_deepseek(image_path, prompt):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": f\"<image_placeholder>{prompt}\",\n",
    "            \"images\": [image_path]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 3. 입력 처리\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation,\n",
    "        images=pil_images,\n",
    "        force_batchify=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 4. 임베딩 생성 (이미지+텍스트)\n",
    "    inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # 5. 답변 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepare_inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# --- 테스트 실행 ---\n",
    "img_path = \"./samples/1.png\" # 이미지 경로 확인\n",
    "\n",
    "print(\"--- DeepSeek-VL2 Test ---\")\n",
    "# 1. 이미지 설명\n",
    "print(\"Caption:\", run_deepseek(img_path, \"Describe this image in detail.\"))\n",
    "\n",
    "# 2. 질문 하기\n",
    "print(\"Answer:\", run_deepseek(img_path, \"What is the main object in the picture?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3787fad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepseek_vl2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepseek_vl2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepseek_vl2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pil_images\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# specify the path to the model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'deepseek_vl2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/deepseek-vl2-tiny\"\n",
    "vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "## single image conversation example\n",
    "## Please note that <|ref|> and <|/ref|> are designed specifically for the object localization feature. These special tokens are not required for normal conversations.\n",
    "## If you would like to experience the grounded captioning functionality (responses that include both object localization and reasoning), you need to add the special token <|grounding|> at the beginning of the prompt. Examples could be found in Figure 9 of our paper.\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"<image>\\n<|ref|>The giraffe at the back.<|/ref|>.\",\n",
    "        \"images\": [\"./images/visual_grounding_1.jpeg\"],\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation,\n",
    "    images=pil_images,\n",
    "    force_batchify=True,\n",
    "    system_prompt=\"\"\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "# run image encoder to get the image embeddings\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "# run the model to get the response\n",
    "outputs = vl_gpt.language.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=False)\n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
